# Glossary

A small glossary of key terms for predictive uncertainty quantification.

## Accuracy
**TODO**: define.

## Aleatoric Uncertainty
From [Wikipedia:](https://en.wikipedia.org/wiki/Uncertainty_quantification#:~:text=Aleatoric%20and%20epistemic%20uncertainty,-Uncertainty%20is%20sometimes&text=Aleatoric%20uncertainty%20is%20also%20known,we%20run%20the%20same%20experiment.&text=Epistemic%20uncertainty%20is%20also%20known,but%20do%20not%20in%20practice.)

> "Aleatoric uncertainty is also known as statistical uncertainty, and is representative of unknowns that differ each time we run the same experiment. For example, a single arrow shot with a mechanical bow that exactly duplicates each launch (the same acceleration, altitude, direction and final velocity) will not all impact the same point on the target due to random and complicated vibrations of the arrow shaft, the knowledge of which cannot be determined sufficiently to eliminate the resulting scatter of impact points."

In other words, this is the uncertainty that is inherent to the system because of information that cannot be measured (i.e. noise). When this noise is present, aleatoric uncertainty cannot be eliminated even as the number of samples collected tends towards infinity.

Examples of aleatoric uncertainty can be found in physical settings where measurement error may exist (e.g. given the same true temperature, a thermometer may output slightly different values), or where inherent noise exist in the system (e.g. a random roll of a die). Taking more temperature measurements will not reduce the uncertainty stemming from an imprecise thermometer, and likewise, observing more rolls of a die will not help us better guess the outcome of a roll, assuming the die is fair.  


## Calibration

### Intuitive Examples
Intuitively, calibration refers to the degree to which a predicted uncertainty matches
the true underlying uncertainty in the data.
For example, if you make a series of 10 predictions about the winning odds of a horse race
 and for each race you predict that a certain horse will win with 70% chance,
if you called the correct winning horse in roughly 7 of the 10 races (70% of the races),
then your uncertainty predictions are said to be calibrated.
The above example is a case of classification with binary outcomes (win or lose).

We can further consider calibration in the regression setting,
which is the focus of the functionalities
of this toolbox.
If you make predictions about the amount of rainfall each day for a whole month, and for each day,
you make a predictive statement, "the amount of rainfall today will not be more than x inches, with 50% chance",
and if your prediction was correct for roughly 15 out of 30 days (50% of the days), then your predictions are said to be calibrated.

### Mathematical Definitions
We focus on the regression setting where the output space is continuous.

#### Notations and Settings
<!--- <img src="https://render.githubusercontent.com/render/math?math="> -->

- <img src="https://render.githubusercontent.com/render/math?math=\mathbf{X, Y}"> are random variables and <img src="https://render.githubusercontent.com/render/math?math=x,y"> are observed values of the random variables.
- Observations for <img src="https://render.githubusercontent.com/render/math?math=\mathbf{Y}"> are always assumed to be conditioned on an observation of <img src="https://render.githubusercontent.com/render/math?math=\mathbf{X}">.
- Dataset <img src="https://render.githubusercontent.com/render/math?math=D = {(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}">, <img src="https://render.githubusercontent.com/render/math?math=x \in \mathbb{R}^{d}, y \in \mathbb{R}">
- The data is generated by a true underlying distribution, characterized by a CDF <img src="https://render.githubusercontent.com/render/math?math=F(\mathbf{Y|X})">, i.e. <img src="https://render.githubusercontent.com/render/math?math=F(k|x) = p_{\mathbf{Y}|x}(\mathbf{Y} \leq k)">
- The inverse function of this true CDF is the true quantile function, <img src="https://render.githubusercontent.com/render/math?math=Q">, <img src="https://render.githubusercontent.com/render/math?math=F(y|x) = p\Leftrightarrow Q(p|x) = y">
- Uncertainty predictions are expressed as predictions of the quantile function <img src="https://render.githubusercontent.com/render/math?math=\hat{Q}(p|x)">

#### Levels of Calibration




=======
> Confidence calibration is "the problem of predicting probability estimates representative of the true correctness likelihood." [(Guo et al., 2017)](https://arxiv.org/pdf/1706.04599.pdf).

Let <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\dpi{300}&space;\hat{Y}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\inline&space;\dpi{300}&space;\hat{Y}" title="\hat{Y}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\dpi{300}&space;\hat{P}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\inline&space;\dpi{300}&space;\hat{P}" title="\hat{P}" /></a> be the predicted class and its associated confidence (probability of correctness). We would like the confidence estimates <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\dpi{300}&space;\hat{P}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\inline&space;\dpi{300}&space;\hat{P}" title="\hat{P}" /></a> to be calibrated, which intuitively means that we want <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\dpi{300}&space;\hat{P}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\inline&space;\dpi{300}&space;\hat{P}" title="\hat{P}" /></a> to represent true probabilities (Guo et al., 2017).

<p align="center">
<a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{300}&space;\mathbb{P}(\hat{Y}=Y&space;\mid&space;\hat{P}=p)=p,&space;\forall&space;p&space;\in[0,1]" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;\mathbb{P}(\hat{Y}=Y&space;\mid&space;\hat{P}=p)=p,&space;\forall&space;p&space;\in[0,1]" title="\mathbb{P}(\hat{Y}=Y \mid \hat{P}=p)=p, \forall p \in[0,1]" /></a>
</p>

Suppose a classification model is given <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\dpi{300}&space;N" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\inline&space;\dpi{300}&space;N" title="N" /></a> input examples, and made predictions <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\dpi{300}&space;\hat{y}_1,&space;...,&space;\hat{y}_N" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\inline&space;\dpi{300}&space;\hat{y}_1,&space;...,&space;\hat{y}_N" title="\hat{y}_1, ..., \hat{y}_N" /></a> , each with <a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\dpi{300}&space;\hat{p}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\inline&space;\dpi{300}&space;\hat{p}" title="\hat{p}" /></a> = `0.35`. We would expect `35%` of the predictions would be correct.


## Confidence
**TODO**: define.

## Epistemic Uncertainty
From [Wikipedia:](https://en.wikipedia.org/wiki/Uncertainty_quantification#:~:text=Aleatoric%20and%20epistemic%20uncertainty,-Uncertainty%20is%20sometimes&text=Aleatoric%20uncertainty%20is%20also%20known,we%20run%20the%20same%20experiment.&text=Epistemic%20uncertainty%20is%20also%20known,but%20do%20not%20in%20practice.)

> "Epistemic uncertainty is also known as systematic uncertainty, and is due to things one could in principle know but do not in practice."

Put another way, epistemic uncertainty is the uncertainty that comes from being unsure about one's model choice. For example, if one is doing modelling with a neural network and is given a finite number of samples to train on, the uncertainty of what the weights in the network should be is epistemic uncertainty. However, as the number of samples being trained on tends to infinity, the epistemic uncertainty tends towards zero as the correct model is able to be identified.

## Predictive Uncertainty
**TODO**: define.

## Sharpness
**TODO**: define.

## Uncertainty
**TODO**: define.
